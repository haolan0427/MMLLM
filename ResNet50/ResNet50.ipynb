{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f5f4a2",
   "metadata": {},
   "source": [
    "数据集增强：\n",
    "\n",
    "①图像增广：翻转（左右、上下）；裁剪；颜色改变（亮度、对比度、饱和度、色调）；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1167931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npython3.10\\ntorch=1.12.1+cu116\\ntorchaudio=0.12.1+cu116\\ntorchinfo=1.8.0\\ntorchvision=0.13.1+cu116\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linux\n",
    "'''\n",
    "Ubuntu 18.04.4 LTS\n",
    "'''\n",
    "\n",
    "# GPU\n",
    "'''\n",
    "CUDA Version: 11.6\n",
    "Driver Version: 510.47.03\n",
    "'''\n",
    "\n",
    "# python和pytorch的环境\n",
    "'''\n",
    "python3.10\n",
    "torch=1.12.1+cu116\n",
    "torchaudio=0.12.1+cu116\n",
    "torchinfo=1.8.0\n",
    "torchvision=0.13.1+cu116\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c82954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorchvision.transforms.Compose() 将图像预处理和增强操作组合在一起\\ntorchvision.transforms.Pad() 对图像进行填充变换\\ntorchvision.transforms.RandomHorizontalFlip() 对图像随机水平翻转\\ntorchvision.transforms.RandomCrop() 随机裁剪图像\\ntorchvision.transforms.ToTensor() 将PIL图像或Numpy数组转换为PyTorch的张量，即Tensor\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入库\n",
    "import math # 用于之后的math.ceil()函数，其能向上取整\n",
    "\n",
    "import torch\n",
    "'''\n",
    "torch.utils.data.DataLoader() 类，数据加载\n",
    "torch.flatten() 将多维张量展平为一维\n",
    "torch.device() # 指定使用GPU还是CPU\n",
    "to.() #将张量或是模型移动到指定的设备上。如果设备是GPU，这样有一系列的好处（加快计算、高效的内存管理、更好的模型性能、便于实验和调试、支持更大的数据集、适应现代深度学习框架）\n",
    "torch.cuda.is_available() 确定是否可以使用GPU\n",
    "torch.optim.Adam() Adam优化算法，结合了Momentum算法和RMSProp算法\n",
    "torch.no_grad() 不进行反向传播\n",
    "torch.max() 得到输入张量中的最值，可以是整个张量的最值，也可以是该张量在某个维度的最值\n",
    "torch.save() 将模型的参数保存到磁盘上\n",
    "'''\n",
    "\n",
    "import torch.nn as nn\n",
    "'''\n",
    "torch.nn.Conv2d 二维卷积层类\n",
    "torch.nn.Module 是一个基类，所有自定义的神经网络模型都要继承自该基类\n",
    "torch.nn.BatchNorm2d 二维批量归一化类\n",
    "torch.nn.ReLU 使用ReLU激活函数\n",
    "torch.nn.Sequential() 指定神经网络线性堆叠结构\n",
    "torch.nn.MaxPool2d() 最大池化操作\n",
    "torch.nn.AdaptiveAvgPool2d() 自适应平均池化层操作\n",
    "torch.nn.Linear() 全连接层\n",
    "torch.nn.CrossEntropyLoss() 在多分类任务中常用的损失函数\n",
    "'''\n",
    "# from torchinfo import summary\n",
    "\n",
    "import torchvision\n",
    "'''\n",
    "torchvision.datasets.CIFAR10() 加载CIFAR-10数据集\n",
    "'''\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "'''\n",
    "torchvision.transforms.Compose() 将图像预处理和增强操作组合在一起\n",
    "torchvision.transforms.Pad() 对图像进行填充变换\n",
    "torchvision.transforms.RandomHorizontalFlip() 对图像随机水平翻转\n",
    "torchvision.transforms.RandomCrop() 随机裁剪图像\n",
    "torchvision.transforms.ToTensor() 将PIL图像或Numpy数组转换为PyTorch的张量，即Tensor\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd52921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 * 32图片\n",
    "img_height = 32\n",
    "img_width = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "123b9c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n将.Pad() .RandomHorizontalFlip() .RandomCrop() .ToTenssor() 四种图像预处理和增强操作顺序执行\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 图片预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4), # 在图像四周都添加4像素\n",
    "    transforms.RandomHorizontalFlip(), # 以概率p=0.5随机水平翻转图像\n",
    "    transforms.RandomCrop(32), # 随机裁剪图像，得到目标尺寸为32x32\n",
    "    transforms.ToTensor() # 将PIL图像转换为Tensor\n",
    "])\n",
    "'''\n",
    "将.Pad() .RandomHorizontalFlip() .RandomCrop() .ToTenssor() 四种图像预处理和增强操作顺序执行\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736c43b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n将CIFAR-10数据集（测试集）保存到/root/data/project/Kaggle/CODE/ResNet50/Dataset/目录下\\n是测试集\\n调用transforms的ToTensor方法，将PIL图像转换为Tensor\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下载数据\n",
    "\n",
    "# 训练数据\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root = \"/root/data/project/Kaggle/CODE/ResNet50/Dataset/\",\n",
    "    train = True,\n",
    "    transform = transform,\n",
    "    download = True\n",
    ")\n",
    "'''\n",
    "将CIFAR-10数据集(训练集)保存到/root/data/project/Kaggle/CODE/ResNet50/Dataset/目录下\n",
    "是训练集\n",
    "调用transform实例来进行数据增强，本例子中对图像进行了：填充、随机水平翻转、随机裁剪和转换为Tensor操作\n",
    "如果本地没有CIFAR-10数据集，则选择下载该数据集\n",
    "'''\n",
    "\n",
    "# 测试数据\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root = \"/root/data/project/Kaggle/CODE/ResNet50/Dataset/\",\n",
    "    train = False,\n",
    "    transform = transforms.ToTensor()\n",
    ")\n",
    "'''\n",
    "将CIFAR-10数据集（测试集）保存到/root/data/project/Kaggle/CODE/ResNet50/Dataset/目录下\n",
    "是测试集\n",
    "调用transforms的ToTensor方法，将PIL图像转换为Tensor\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a65453f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "num_epochs = 20\n",
    "batch_size = 50\n",
    "learning_rate= 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afe61de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndataset 导入的数据集\\nbatch_size 一个迭代(iteration)所前馈、反馈和更新处理的样本数量\\nshuffle 在每一个轮次(epoch)前，将所有样本打乱\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入数据\n",
    "\n",
    "# 训练数据\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "# 测试数据\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "'''\n",
    "dataset 导入的数据集\n",
    "batch_size 一个迭代(iteration)所前馈、反馈和更新处理的样本数量\n",
    "shuffle 在每一个轮次(epoch)前，将所有样本打乱\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09728609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到一个卷积层实例\n",
    "def conv3x3(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "190a0f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual block\n",
    "class ResidualBlock(nn.Module): # 自定义神经网络模型，ResidualBlock类需要继承自torch.nn.Module类\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.mid_channels = out_channels//4\n",
    "        self.conv1 = conv3x3(in_channels, self.mid_channels, kernel_size=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(self.mid_channels) # self.mid_channels是卷积层输出通道数，self.mid_channels个输出通道的批量归一化\n",
    "        self.relu = nn.ReLU(inplace=True) # 构造relu实例，令inplacce=True，反向传播不用原始值\n",
    "        self.conv2 = conv3x3(self.mid_channels, self.mid_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(self.mid_channels) # self.mid_channels是卷积层输出通道数，self.mid_channels个输出通道的批量归一化\n",
    "        self.conv3 = conv3x3(self.mid_channels, out_channels, kernel_size=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels) # out_channels是卷积层输出通道数，out_channels个输出通道的批量归一化\n",
    "        self.downsample_0 = nn.Sequential() # 定义空容器实例downsample_0\n",
    "        self.downsample = downsample\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out) # 使用relu激活函数\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out) # 使用relu激活函数\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        else:\n",
    "            residual = self.downsample_0(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out) # 使用relu激活函数\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec6d46dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "\n",
    "# ResNet50\n",
    "class ResNet(nn.Module): # 自定义神经网络模型，ResNet类需要继承自基类torch.nn.Module\n",
    "    def __init__(self, block, layers, num_classes = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv = conv3x3(3, 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn = nn.BatchNorm2d(64) # 64是卷积层输出通道数，64个输出通道的批量归一化\n",
    "        self.max_pool = nn.MaxPool2d(3, 2, padding = 1) # 最大池化操作，kernel_size=3x3;stride=2;padding=1\n",
    "        self.relu = nn.ReLU(inplace = True) # 构造relu实例，令inplace=True，反向传播中不用始值\n",
    "        self.layer1 = self.make_layer(block, 64, 256, layers[0], 1)\n",
    "        self.layer2 = self.make_layer(block, 256, 512, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 512, 1024, layers[2], 2)\n",
    "        self.layer4 = self.make_layer(block, 1024, 2048, layers[3], 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) # 进行自适应平均池化层操作，输出高度x宽度为1x1\n",
    "        self.fc = nn.Linear(math.ceil(img_height / 32) * math.ceil(img_width / 32) * 2048, num_classes)  # 构造全连接层实例fc，用于神经网络的最后阶段，将特征图展平并映射到输出类别(2048, 10)\n",
    "\n",
    "    def make_layer(self, block, in_channels, out_channels, blocks, stride = 1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (in_channels != out_channels):\n",
    "            downsample = nn.Sequential( # 定义容器实例downsample，是线性的：conv3x3;BatchNorm2d;\n",
    "                conv3x3(in_channels, out_channels, kernel_size = 1, stride = stride, padding = 0),\n",
    "                nn.BatchNorm2d(out_channels) # out_channels是卷积层输出通道数，out_channels个输出通道的批量归一化\n",
    "            )\n",
    "        layers = [block(in_channels, out_channels, stride, downsample)]\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers) # 将layers线性化连接起来\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.max_pool(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = torch.flatten(out, 1) # 通道维数不变，在每一通道维度的基础上，展平第二和第三维度\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ea9b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有GPU则device赋为\"cuda\"，即指定默认的GPU；如果没有GPU则使用CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e079316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型实例化，并且将模型移动到指定的设备上\n",
    "model = ResNet(ResidualBlock, [3, 4, 6, 3]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfc86175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失实例，是交叉熵损失实例\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df4e294e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化实例，使用Adam优化算法，第一个参数模型实例的参数；第二个参数是学习率，是一个超参数，需要自己设定\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69b95148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新学习率\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f391fd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/20], Step [100/1000] Loss:2.2465\n",
      "Epoch[1/20], Step [200/1000] Loss:2.3048\n",
      "Epoch[1/20], Step [300/1000] Loss:2.0614\n",
      "Epoch[1/20], Step [400/1000] Loss:1.8806\n",
      "Epoch[1/20], Step [500/1000] Loss:1.8562\n",
      "Epoch[1/20], Step [600/1000] Loss:1.8562\n",
      "Epoch[1/20], Step [700/1000] Loss:1.9196\n",
      "Epoch[1/20], Step [800/1000] Loss:1.7620\n",
      "Epoch[1/20], Step [900/1000] Loss:1.5524\n",
      "Epoch[1/20], Step [1000/1000] Loss:1.5996\n",
      "Epoch[2/20], Step [100/1000] Loss:1.7130\n",
      "Epoch[2/20], Step [200/1000] Loss:1.7179\n",
      "Epoch[2/20], Step [300/1000] Loss:1.5812\n",
      "Epoch[2/20], Step [400/1000] Loss:1.5514\n",
      "Epoch[2/20], Step [500/1000] Loss:1.4996\n",
      "Epoch[2/20], Step [600/1000] Loss:1.7425\n",
      "Epoch[2/20], Step [700/1000] Loss:1.4656\n",
      "Epoch[2/20], Step [800/1000] Loss:1.6440\n",
      "Epoch[2/20], Step [900/1000] Loss:1.6708\n",
      "Epoch[2/20], Step [1000/1000] Loss:1.4362\n",
      "Epoch[3/20], Step [100/1000] Loss:1.7464\n",
      "Epoch[3/20], Step [200/1000] Loss:1.2145\n",
      "Epoch[3/20], Step [300/1000] Loss:1.5714\n",
      "Epoch[3/20], Step [400/1000] Loss:1.4206\n",
      "Epoch[3/20], Step [500/1000] Loss:1.4701\n",
      "Epoch[3/20], Step [600/1000] Loss:1.1825\n",
      "Epoch[3/20], Step [700/1000] Loss:1.4984\n",
      "Epoch[3/20], Step [800/1000] Loss:1.3705\n",
      "Epoch[3/20], Step [900/1000] Loss:1.2141\n",
      "Epoch[3/20], Step [1000/1000] Loss:1.0794\n",
      "Epoch[4/20], Step [100/1000] Loss:1.3309\n",
      "Epoch[4/20], Step [200/1000] Loss:1.3159\n",
      "Epoch[4/20], Step [300/1000] Loss:1.1502\n",
      "Epoch[4/20], Step [400/1000] Loss:1.2599\n",
      "Epoch[4/20], Step [500/1000] Loss:1.3154\n",
      "Epoch[4/20], Step [600/1000] Loss:1.1602\n",
      "Epoch[4/20], Step [700/1000] Loss:1.1118\n",
      "Epoch[4/20], Step [800/1000] Loss:0.9712\n",
      "Epoch[4/20], Step [900/1000] Loss:1.2404\n",
      "Epoch[4/20], Step [1000/1000] Loss:1.3372\n",
      "Epoch[5/20], Step [100/1000] Loss:1.3497\n",
      "Epoch[5/20], Step [200/1000] Loss:1.2942\n",
      "Epoch[5/20], Step [300/1000] Loss:1.0052\n",
      "Epoch[5/20], Step [400/1000] Loss:1.3205\n",
      "Epoch[5/20], Step [500/1000] Loss:1.3551\n",
      "Epoch[5/20], Step [600/1000] Loss:1.0389\n",
      "Epoch[5/20], Step [700/1000] Loss:1.2244\n",
      "Epoch[5/20], Step [800/1000] Loss:1.3005\n",
      "Epoch[5/20], Step [900/1000] Loss:1.2197\n",
      "Epoch[5/20], Step [1000/1000] Loss:1.1539\n",
      "Epoch[6/20], Step [100/1000] Loss:1.0747\n",
      "Epoch[6/20], Step [200/1000] Loss:1.1399\n",
      "Epoch[6/20], Step [300/1000] Loss:1.1227\n",
      "Epoch[6/20], Step [400/1000] Loss:1.1307\n",
      "Epoch[6/20], Step [500/1000] Loss:1.0199\n",
      "Epoch[6/20], Step [600/1000] Loss:1.1988\n",
      "Epoch[6/20], Step [700/1000] Loss:1.0861\n",
      "Epoch[6/20], Step [800/1000] Loss:1.0066\n",
      "Epoch[6/20], Step [900/1000] Loss:0.9573\n",
      "Epoch[6/20], Step [1000/1000] Loss:1.0509\n",
      "Epoch[7/20], Step [100/1000] Loss:0.9041\n",
      "Epoch[7/20], Step [200/1000] Loss:1.1010\n",
      "Epoch[7/20], Step [300/1000] Loss:0.9024\n",
      "Epoch[7/20], Step [400/1000] Loss:1.0344\n",
      "Epoch[7/20], Step [500/1000] Loss:0.9750\n",
      "Epoch[7/20], Step [600/1000] Loss:1.1491\n",
      "Epoch[7/20], Step [700/1000] Loss:0.8480\n",
      "Epoch[7/20], Step [800/1000] Loss:0.8528\n",
      "Epoch[7/20], Step [900/1000] Loss:0.8475\n",
      "Epoch[7/20], Step [1000/1000] Loss:1.0146\n",
      "Epoch[8/20], Step [100/1000] Loss:0.7343\n",
      "Epoch[8/20], Step [200/1000] Loss:0.6287\n",
      "Epoch[8/20], Step [300/1000] Loss:0.8524\n",
      "Epoch[8/20], Step [400/1000] Loss:1.0743\n",
      "Epoch[8/20], Step [500/1000] Loss:1.0194\n",
      "Epoch[8/20], Step [600/1000] Loss:0.9841\n",
      "Epoch[8/20], Step [700/1000] Loss:0.7046\n",
      "Epoch[8/20], Step [800/1000] Loss:0.7372\n",
      "Epoch[8/20], Step [900/1000] Loss:0.6309\n",
      "Epoch[8/20], Step [1000/1000] Loss:0.8207\n",
      "Epoch[9/20], Step [100/1000] Loss:0.7294\n",
      "Epoch[9/20], Step [200/1000] Loss:1.4193\n",
      "Epoch[9/20], Step [300/1000] Loss:1.2019\n",
      "Epoch[9/20], Step [400/1000] Loss:0.8695\n",
      "Epoch[9/20], Step [500/1000] Loss:0.9293\n",
      "Epoch[9/20], Step [600/1000] Loss:0.8853\n",
      "Epoch[9/20], Step [700/1000] Loss:1.0512\n",
      "Epoch[9/20], Step [800/1000] Loss:0.9103\n",
      "Epoch[9/20], Step [900/1000] Loss:0.7561\n",
      "Epoch[9/20], Step [1000/1000] Loss:0.8475\n",
      "Epoch[10/20], Step [100/1000] Loss:0.6159\n",
      "Epoch[10/20], Step [200/1000] Loss:1.1154\n",
      "Epoch[10/20], Step [300/1000] Loss:0.5430\n",
      "Epoch[10/20], Step [400/1000] Loss:0.7297\n",
      "Epoch[10/20], Step [500/1000] Loss:0.7656\n",
      "Epoch[10/20], Step [600/1000] Loss:0.8631\n",
      "Epoch[10/20], Step [700/1000] Loss:0.7819\n",
      "Epoch[10/20], Step [800/1000] Loss:0.6333\n",
      "Epoch[10/20], Step [900/1000] Loss:0.9871\n",
      "Epoch[10/20], Step [1000/1000] Loss:0.6989\n",
      "Epoch[11/20], Step [100/1000] Loss:0.8121\n",
      "Epoch[11/20], Step [200/1000] Loss:0.8465\n",
      "Epoch[11/20], Step [300/1000] Loss:0.8546\n",
      "Epoch[11/20], Step [400/1000] Loss:0.7408\n",
      "Epoch[11/20], Step [500/1000] Loss:0.6887\n",
      "Epoch[11/20], Step [600/1000] Loss:0.8202\n",
      "Epoch[11/20], Step [700/1000] Loss:0.8408\n",
      "Epoch[11/20], Step [800/1000] Loss:0.7047\n",
      "Epoch[11/20], Step [900/1000] Loss:0.6392\n",
      "Epoch[11/20], Step [1000/1000] Loss:0.5285\n",
      "Epoch[12/20], Step [100/1000] Loss:0.5512\n",
      "Epoch[12/20], Step [200/1000] Loss:0.5347\n",
      "Epoch[12/20], Step [300/1000] Loss:0.5783\n",
      "Epoch[12/20], Step [400/1000] Loss:0.7012\n",
      "Epoch[12/20], Step [500/1000] Loss:0.4986\n",
      "Epoch[12/20], Step [600/1000] Loss:0.6538\n",
      "Epoch[12/20], Step [700/1000] Loss:0.6138\n",
      "Epoch[12/20], Step [800/1000] Loss:0.7788\n",
      "Epoch[12/20], Step [900/1000] Loss:0.4573\n",
      "Epoch[12/20], Step [1000/1000] Loss:0.7564\n",
      "Epoch[13/20], Step [100/1000] Loss:0.5404\n",
      "Epoch[13/20], Step [200/1000] Loss:0.6047\n",
      "Epoch[13/20], Step [300/1000] Loss:0.6731\n",
      "Epoch[13/20], Step [400/1000] Loss:0.6849\n",
      "Epoch[13/20], Step [500/1000] Loss:0.5624\n",
      "Epoch[13/20], Step [600/1000] Loss:0.5795\n",
      "Epoch[13/20], Step [700/1000] Loss:0.6336\n",
      "Epoch[13/20], Step [800/1000] Loss:0.5519\n",
      "Epoch[13/20], Step [900/1000] Loss:0.4777\n",
      "Epoch[13/20], Step [1000/1000] Loss:0.7387\n",
      "Epoch[14/20], Step [100/1000] Loss:0.7240\n",
      "Epoch[14/20], Step [200/1000] Loss:0.4377\n",
      "Epoch[14/20], Step [300/1000] Loss:0.5222\n",
      "Epoch[14/20], Step [400/1000] Loss:0.6702\n",
      "Epoch[14/20], Step [500/1000] Loss:0.5003\n",
      "Epoch[14/20], Step [600/1000] Loss:0.7336\n",
      "Epoch[14/20], Step [700/1000] Loss:0.6483\n",
      "Epoch[14/20], Step [800/1000] Loss:0.5884\n",
      "Epoch[14/20], Step [900/1000] Loss:0.4193\n",
      "Epoch[14/20], Step [1000/1000] Loss:0.6701\n",
      "Epoch[15/20], Step [100/1000] Loss:0.6744\n",
      "Epoch[15/20], Step [200/1000] Loss:0.7203\n",
      "Epoch[15/20], Step [300/1000] Loss:0.6524\n",
      "Epoch[15/20], Step [400/1000] Loss:0.6824\n",
      "Epoch[15/20], Step [500/1000] Loss:0.6575\n",
      "Epoch[15/20], Step [600/1000] Loss:0.3829\n",
      "Epoch[15/20], Step [700/1000] Loss:0.6260\n",
      "Epoch[15/20], Step [800/1000] Loss:0.4980\n",
      "Epoch[15/20], Step [900/1000] Loss:0.6440\n",
      "Epoch[15/20], Step [1000/1000] Loss:0.3522\n",
      "Epoch[16/20], Step [100/1000] Loss:0.5026\n",
      "Epoch[16/20], Step [200/1000] Loss:0.5036\n",
      "Epoch[16/20], Step [300/1000] Loss:0.3486\n",
      "Epoch[16/20], Step [400/1000] Loss:0.7649\n",
      "Epoch[16/20], Step [500/1000] Loss:0.7801\n",
      "Epoch[16/20], Step [600/1000] Loss:0.5569\n",
      "Epoch[16/20], Step [700/1000] Loss:0.4883\n",
      "Epoch[16/20], Step [800/1000] Loss:0.6328\n",
      "Epoch[16/20], Step [900/1000] Loss:0.4908\n",
      "Epoch[16/20], Step [1000/1000] Loss:0.5599\n",
      "Epoch[17/20], Step [100/1000] Loss:0.4448\n",
      "Epoch[17/20], Step [200/1000] Loss:0.4323\n",
      "Epoch[17/20], Step [300/1000] Loss:0.4149\n",
      "Epoch[17/20], Step [400/1000] Loss:0.3399\n",
      "Epoch[17/20], Step [500/1000] Loss:0.5793\n",
      "Epoch[17/20], Step [600/1000] Loss:0.4083\n",
      "Epoch[17/20], Step [700/1000] Loss:0.4650\n",
      "Epoch[17/20], Step [800/1000] Loss:0.4717\n",
      "Epoch[17/20], Step [900/1000] Loss:0.6402\n",
      "Epoch[17/20], Step [1000/1000] Loss:0.5431\n",
      "Epoch[18/20], Step [100/1000] Loss:0.2943\n",
      "Epoch[18/20], Step [200/1000] Loss:0.4160\n",
      "Epoch[18/20], Step [300/1000] Loss:0.5383\n",
      "Epoch[18/20], Step [400/1000] Loss:0.3647\n",
      "Epoch[18/20], Step [500/1000] Loss:0.3058\n",
      "Epoch[18/20], Step [600/1000] Loss:0.4413\n",
      "Epoch[18/20], Step [700/1000] Loss:0.6840\n",
      "Epoch[18/20], Step [800/1000] Loss:0.3949\n",
      "Epoch[18/20], Step [900/1000] Loss:0.4393\n",
      "Epoch[18/20], Step [1000/1000] Loss:0.9213\n",
      "Epoch[19/20], Step [100/1000] Loss:0.5040\n",
      "Epoch[19/20], Step [200/1000] Loss:0.3072\n",
      "Epoch[19/20], Step [300/1000] Loss:0.5151\n",
      "Epoch[19/20], Step [400/1000] Loss:0.6464\n",
      "Epoch[19/20], Step [500/1000] Loss:0.3658\n",
      "Epoch[19/20], Step [600/1000] Loss:0.5853\n",
      "Epoch[19/20], Step [700/1000] Loss:0.4295\n",
      "Epoch[19/20], Step [800/1000] Loss:0.4876\n",
      "Epoch[19/20], Step [900/1000] Loss:0.3879\n",
      "Epoch[19/20], Step [1000/1000] Loss:0.4128\n",
      "Epoch[20/20], Step [100/1000] Loss:0.3950\n",
      "Epoch[20/20], Step [200/1000] Loss:0.3938\n",
      "Epoch[20/20], Step [300/1000] Loss:0.3658\n",
      "Epoch[20/20], Step [400/1000] Loss:0.5202\n",
      "Epoch[20/20], Step [500/1000] Loss:0.3904\n",
      "Epoch[20/20], Step [600/1000] Loss:0.4544\n",
      "Epoch[20/20], Step [700/1000] Loss:0.4052\n",
      "Epoch[20/20], Step [800/1000] Loss:0.3832\n",
      "Epoch[20/20], Step [900/1000] Loss:0.4525\n",
      "Epoch[20/20], Step [1000/1000] Loss:0.6060\n"
     ]
    }
   ],
   "source": [
    "# 训练函数\n",
    "total_step = len(train_loader) # total_step = 训练集总样本数 // batch_size = 50,000 // 50 = 1,000\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epochs): # num_epochs轮次数\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device) # 将张量images移动到指定的设备中\n",
    "        labels = labels.to(device) # 将张量labels移动到指定的\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"Epoch[{}/{}], Step [{}/{}] Loss:{:.4f}\".format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1cb8706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test image:81.95\n"
     ]
    }
   ],
   "source": [
    "# 测试函数\n",
    "model.eval() # 进入模型评估模式，禁用Dropout、禁用Batch Normalization\n",
    "with torch.no_grad(): # 在此缩进块中，不再进行反向传播\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device) # 将张量images移动到指定的设备中\n",
    "        labels = labels.to(device) # 将张量labels移动到指定的设备中\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1) # 返回输出最大值和索引，索引即预测的类别\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(\"Accuracy of the model on the test image:{}\".format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf33733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数到/root/data/project/Kaggle/CODE/ResNet50/Dataset/目录下，文件名为resnet50_cifar10.pth\n",
    "torch.save(model.state_dict(), \"Dataset/resnet50_cifar10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24683738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc85300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc350b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823f1564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469be65f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f13eab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
